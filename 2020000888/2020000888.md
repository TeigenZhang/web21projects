# Pre-training based Multilingual VQA
Zhangliang's Web class project (student id: 2020000888)

Vision Question Answering (VQA) is one of the most challenging Vision & Language tasks. Given an image and a question about the content, VQA aims to answer the question. It requires the model not only to understand both image and language, but also to have the common sense reasoning ability. Many studies is conducted to develop VQA models. But due to the lack of datasets, most of them build their models with English-only VQA datasets. 

In this project, we extend VQA to multilingual scenario using pre-training. We pre-trained the model with image-text pairs crawled from the web. The pre-training dataset is extended to 7 languages using machine translation. We then finetune the pre-trained model with two widely used VQA datasets `VQA 2.0 (English)` and `VQA VG Japanese (Japanese)`. Result shows that our model could achieve significant growth compared with the state-of-the-art models without pre-training. 