# Pre-training based Multilingual VQA

Zhangliang's Web class project (student id: 2020000888)

## Introduction
Vision Question Answering (VQA) is one of the most challenging Vision & Language tasks. Given an image and a question about the content, VQA aims to answer the question. It requires the model not only to understand both image and language, but also to have the common sense reasoning ability. Many studies is conducted to develop VQA models. But due to the lack of datasets, most of them build their models with English-only VQA datasets. 

In this project, we extend VQA to multilingual scenario using pre-training. We pre-trained the model with image-text pairs crawled from the web. The pre-training dataset is extended to 7 languages using machine translation. We then finetune the pre-trained model with two widely used VQA datasets `VQA 2.0 (English)` and `VQA VG Japanese (Japanese)`. Result shows that our model could achieve significant growth compared with the state-of-the-art models without pre-training. Besides, visulization experiments shows that our model could attend to the relative image region given an question with different languages, which shows the interpretation and generalize ablity of the model. 

This project is based on [UNITER](https://arxiv.org/abs/1909.11740) (ECCV 2020).


## Requirements
We provide Docker image for easier reproduction. Please install the following:
  - [nvidia driver](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-installation) (418+), 
  - [Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/) (19.03+), 
  - [nvidia-container-toolkit](https://github.com/NVIDIA/nvidia-docker#quickstart).

Our scripts require the user to have the [docker group membership](https://docs.docker.com/install/linux/linux-postinstall/)
so that docker commands can be run without sudo.
We only support Linux with NVIDIA GPUs. We test on Ubuntu 18.04 and V100 cards.
We use mixed-precision training hence GPUs with Tensor Cores are recommended.

## How to use

NOTE: train and inference should be ran inside the docker container

1. train  
    For `VQA2.0 (English)`:
    ```
    horovodrun -np 1 python train_vqa.py --config config/multilingual/vqa/train-xlmr-large.json \
        --output_dir $VQA_EXP
    ```
    For `VG VQA JA (Japanese)`:
    ```
    horovodrun -np 1 python train_vqa.py --config config/multilingual/vqa/train-xlmr-large-ja.json \
        --output_dir $VQA_EXP
    ```
2. inference
    ```
    python inf_vqa.py --txt_db /txt/vqa_test.db --img_db /img/coco_test2015 \
        --output_dir $VQA_EXP --checkpoint 6000 --pin_mem --fp16
    ```
    The result file will be written at `$VQA_EXP/results_test/results_6000_all.json`, which can be
    submitted to the evaluation server
